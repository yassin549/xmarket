"use strict";
/**
 * Embedding Batcher
 *
 * Batches text for embedding generation and stores to Pinecone.
 * Implements queue-based processing with configurable batch size.
 */
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __generator = (this && this.__generator) || function (thisArg, body) {
    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g = Object.create((typeof Iterator === "function" ? Iterator : Object).prototype);
    return g.next = verb(0), g["throw"] = verb(1), g["return"] = verb(2), typeof Symbol === "function" && (g[Symbol.iterator] = function() { return this; }), g;
    function verb(n) { return function (v) { return step([n, v]); }; }
    function step(op) {
        if (f) throw new TypeError("Generator is already executing.");
        while (g && (g = 0, op[0] && (_ = 0)), _) try {
            if (f = 1, y && (t = op[0] & 2 ? y["return"] : op[0] ? y["throw"] || ((t = y["return"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;
            if (y = 0, t) op = [op[0] & 2, t.value];
            switch (op[0]) {
                case 0: case 1: t = op; break;
                case 4: _.label++; return { value: op[1], done: false };
                case 5: _.label++; y = op[1]; op = [0]; continue;
                case 7: op = _.ops.pop(); _.trys.pop(); continue;
                default:
                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }
                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }
                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }
                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }
                    if (t[2]) _.ops.pop();
                    _.trys.pop(); continue;
            }
            op = body.call(thisArg, _);
        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }
        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };
    }
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.EmbeddingBatcher = void 0;
var pinecone_1 = require("@pinecone-database/pinecone");
var hf_client_1 = __importDefault(require("../../infra/llm/hf_client"));
var EmbeddingBatcher = /** @class */ (function () {
    function EmbeddingBatcher(config) {
        if (config === void 0) { config = {}; }
        this.queue = [];
        this.processing = false;
        this.batchSize = config.batchSize || parseInt(process.env.EMBED_BATCH_SIZE || '32');
        this.maxRetries = config.maxRetries || parseInt(process.env.EMBED_MAX_RETRIES || '3');
        this.retryDelay = config.retryDelay || parseInt(process.env.EMBED_RETRY_DELAY || '5000');
        // Initialize Pinecone
        var apiKey = process.env.PINECONE_API_KEY;
        if (!apiKey) {
            throw new Error('PINECONE_API_KEY not configured');
        }
        this.pinecone = new pinecone_1.Pinecone({ apiKey: apiKey });
        this.hfClient = new hf_client_1.default();
    }
    /**
     * Initialize Pinecone index connection
     */
    EmbeddingBatcher.prototype.initialize = function () {
        return __awaiter(this, void 0, void 0, function () {
            var indexName, indexHost;
            return __generator(this, function (_a) {
                indexName = process.env.PINECONE_INDEX_NAME || 'xmarket';
                indexHost = process.env.PINECONE_INDEX_HOST;
                if (!indexHost) {
                    throw new Error('PINECONE_INDEX_HOST not configured');
                }
                this.index = this.pinecone.index(indexName, indexHost);
                console.log("Pinecone index initialized: ".concat(indexName));
                return [2 /*return*/];
            });
        });
    };
    /**
     * Add text to embedding queue
     *
     * Auto-processes when batch is full
     */
    EmbeddingBatcher.prototype.enqueue = function (job) {
        return __awaiter(this, void 0, void 0, function () {
            return __generator(this, function (_a) {
                switch (_a.label) {
                    case 0:
                        this.queue.push(job);
                        console.log("Enqueued embedding job (queue size: ".concat(this.queue.length, "/").concat(this.batchSize, ")"));
                        if (!(this.queue.length >= this.batchSize && !this.processing)) return [3 /*break*/, 2];
                        return [4 /*yield*/, this.processBatch()];
                    case 1:
                        _a.sent();
                        _a.label = 2;
                    case 2: return [2 /*return*/];
                }
            });
        });
    };
    /**
     * Process batch of embeddings
     *
     * Extracts batch, generates embeddings via HF, stores to Pinecone
     */
    EmbeddingBatcher.prototype.processBatch = function () {
        return __awaiter(this, void 0, void 0, function () {
            var batch, embeddings_1, attempt, error_1, vectors, error_2;
            return __generator(this, function (_a) {
                switch (_a.label) {
                    case 0:
                        if (this.queue.length === 0 || this.processing) {
                            return [2 /*return*/];
                        }
                        this.processing = true;
                        _a.label = 1;
                    case 1:
                        _a.trys.push([1, 10, 11, 12]);
                        batch = this.queue.splice(0, this.batchSize);
                        console.log("Processing batch of ".concat(batch.length, " embeddings"));
                        attempt = 0;
                        _a.label = 2;
                    case 2:
                        if (!(attempt < this.maxRetries)) return [3 /*break*/, 8];
                        _a.label = 3;
                    case 3:
                        _a.trys.push([3, 5, , 7]);
                        return [4 /*yield*/, this.hfClient.generateEmbeddings(batch.map(function (j) { return j.text; }))];
                    case 4:
                        embeddings_1 = _a.sent();
                        return [3 /*break*/, 8]; // Success
                    case 5:
                        error_1 = _a.sent();
                        attempt++;
                        if (attempt >= this.maxRetries) {
                            throw error_1; // Give up
                        }
                        console.warn("Embedding generation failed (attempt ".concat(attempt, "/").concat(this.maxRetries, "):"), error_1);
                        // Exponential backoff
                        return [4 /*yield*/, this.sleep(this.retryDelay * Math.pow(2, attempt - 1))];
                    case 6:
                        // Exponential backoff
                        _a.sent();
                        return [3 /*break*/, 7];
                    case 7: return [3 /*break*/, 2];
                    case 8:
                        vectors = batch.map(function (job, i) { return ({
                            id: "".concat(job.metadata.snapshot_id, "-").concat(i, "-").concat(Date.now()),
                            values: embeddings_1[i],
                            metadata: {
                                ingest_id: job.metadata.ingest_id,
                                snapshot_id: job.metadata.snapshot_id,
                                url: job.metadata.url,
                                fetched_at: job.metadata.fetched_at,
                                content_type: job.metadata.content_type || 'text/html',
                                text_preview: job.text.substring(0, 500),
                                indexed_at: new Date().toISOString(),
                            },
                        }); });
                        // Upsert to Pinecone
                        return [4 /*yield*/, this.index.upsert(vectors)];
                    case 9:
                        // Upsert to Pinecone
                        _a.sent();
                        console.log("\u2705 Stored ".concat(vectors.length, " embeddings to Pinecone"));
                        return [3 /*break*/, 12];
                    case 10:
                        error_2 = _a.sent();
                        console.error('Batch processing error:', error_2);
                        return [3 /*break*/, 12];
                    case 11:
                        this.processing = false;
                        return [7 /*endfinally*/];
                    case 12: return [2 /*return*/];
                }
            });
        });
    };
    /**
     * Flush remaining queue
     *
     * Process all remaining items regardless of batch size
     */
    EmbeddingBatcher.prototype.flush = function () {
        return __awaiter(this, void 0, void 0, function () {
            return __generator(this, function (_a) {
                switch (_a.label) {
                    case 0:
                        console.log("Flushing queue (".concat(this.queue.length, " items remaining)"));
                        _a.label = 1;
                    case 1:
                        if (!(this.queue.length > 0)) return [3 /*break*/, 3];
                        return [4 /*yield*/, this.processBatch()];
                    case 2:
                        _a.sent();
                        return [3 /*break*/, 1];
                    case 3:
                        console.log('Queue flushed');
                        return [2 /*return*/];
                }
            });
        });
    };
    /**
     * Get queue stats
     */
    EmbeddingBatcher.prototype.getStats = function () {
        return {
            queueSize: this.queue.length,
            batchSize: this.batchSize,
            processing: this.processing,
        };
    };
    /**
     * Query Pinecone for similar embeddings
     *
     * @param query Query text
     * @param topK Number of results
     * @param filter Metadata filter
     */
    EmbeddingBatcher.prototype.query = function (query_1) {
        return __awaiter(this, arguments, void 0, function (query, topK, filter) {
            var queryEmbedding, result;
            if (topK === void 0) { topK = 10; }
            return __generator(this, function (_a) {
                switch (_a.label) {
                    case 0: return [4 /*yield*/, this.hfClient.generateEmbeddings([query])];
                    case 1:
                        queryEmbedding = (_a.sent())[0];
                        return [4 /*yield*/, this.index.query({
                                vector: queryEmbedding,
                                topK: topK,
                                filter: filter,
                                includeMetadata: true,
                            })];
                    case 2:
                        result = _a.sent();
                        return [2 /*return*/, result];
                }
            });
        });
    };
    /**
     * Sleep helper for retry backoff
     */
    EmbeddingBatcher.prototype.sleep = function (ms) {
        return new Promise(function (resolve) { return setTimeout(resolve, ms); });
    };
    return EmbeddingBatcher;
}());
exports.EmbeddingBatcher = EmbeddingBatcher;
/**
 * Default export
 */
exports.default = EmbeddingBatcher;
